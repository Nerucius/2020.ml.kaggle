{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition\n",
    "## Any Bossom at Home?\n",
    "\n",
    "We have been asked by CERN physicists to help them uncover important particle traces. We are provided data from the Large Hadron Collider B, where protons collide at 99.9999% of the speed of the light. There are 40M of collisions per second, and each collision produces from hundreds to thousands of particles. From those only a very small set are of interest. We are looking for a process of B0 into K*0 gamma decomposition. Only 10 of these processes are produced every our. Can we uncover them from the sea of processes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model, ensemble, tree, svm, neighbors, neural_network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, QuantileTransformer, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and Split into train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['B_OWNPV_CHI2', 'B_IPCHI2_OWNPV', 'B_FDCHI2_OWNPV', 'B_DIRA_OWNPV',\n",
      "       'B_PT', 'Kst_892_0_IP_OWNPV', 'Kst_892_0_cosThetaH', 'Kplus_IP_OWNPV',\n",
      "       'Kplus_P', 'piminus_IP_OWNPV', 'piminus_P', 'gamma_PT', 'piminus_ETA',\n",
      "       'Kplus_ETA', 'BUTTER', 'signal'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    141632\n",
       "1.0     71030\n",
       "Name: signal, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv', index_col=0)\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.columns)\n",
    "\n",
    "# Low separation\n",
    "# del df['piminus_ETA']\n",
    "# del df['Kplus_ETA']\n",
    "\n",
    "# WTF is this?\n",
    "del df['BUTTER']\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression();None;0.734608;0.787417\n",
      "LinearRegression();Normalizer();0.743009;0.799472\n",
      "LinearRegression();MinMaxScaler();0.734608;0.787417\n",
      "LinearRegression();StandardScaler();0.734608;0.787417\n",
      "LinearRegression();QuantileTransformer();0.756834;0.817446\n",
      "LinearRegression();RobustScaler();0.734608;0.787417\n",
      "RidgeClassifier();None;0.734545;0.658992\n",
      "RidgeClassifier();Normalizer();0.684013;0.561407\n",
      "RidgeClassifier();MinMaxScaler();0.734828;0.659507\n",
      "RidgeClassifier();StandardScaler();0.734608;0.659435\n",
      "RidgeClassifier();QuantileTransformer();0.756740;0.695968\n",
      "RidgeClassifier();RobustScaler();0.734608;0.659435\n",
      "DecisionTreeClassifier();None;0.690784;0.655502\n",
      "DecisionTreeClassifier();Normalizer();0.675643;0.639073\n",
      "DecisionTreeClassifier();MinMaxScaler();0.692226;0.657331\n",
      "DecisionTreeClassifier();StandardScaler();0.689687;0.653909\n",
      "DecisionTreeClassifier();QuantileTransformer();0.688119;0.652499\n",
      "DecisionTreeClassifier();RobustScaler();0.687774;0.651844\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);None;0.772006;0.838596\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);Normalizer();0.757806;0.822759\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);MinMaxScaler();0.772038;0.839163\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);StandardScaler();0.772163;0.838939\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);QuantileTransformer();0.771567;0.839426\n",
      "RandomForestClassifier(class_weight={0: 2, 1: 3}, min_samples_leaf=5);RobustScaler();0.770972;0.839117\n",
      "AdaBoostClassifier();None;0.752821;0.816130\n",
      "AdaBoostClassifier();Normalizer();0.694671;0.730214\n",
      "AdaBoostClassifier();MinMaxScaler();0.757273;0.820566\n",
      "AdaBoostClassifier();StandardScaler();0.757147;0.820512\n",
      "AdaBoostClassifier();QuantileTransformer();0.757116;0.820524\n",
      "AdaBoostClassifier();RobustScaler();0.757147;0.820512\n",
      "GradientBoostingClassifier();None;0.760094;0.824886\n",
      "GradientBoostingClassifier();Normalizer();0.720063;0.771464\n",
      "GradientBoostingClassifier();MinMaxScaler();0.762853;0.828263\n",
      "GradientBoostingClassifier();StandardScaler();0.763135;0.828442\n",
      "GradientBoostingClassifier();QuantileTransformer();0.762978;0.828602\n",
      "GradientBoostingClassifier();RobustScaler();0.762978;0.828602\n",
      "KNeighborsClassifier();None;0.671442;0.670422\n",
      "KNeighborsClassifier();Normalizer();0.659906;0.653687\n",
      "KNeighborsClassifier();MinMaxScaler();0.731912;0.765957\n",
      "KNeighborsClassifier();StandardScaler();0.727116;0.762093\n",
      "KNeighborsClassifier();QuantileTransformer();0.732069;0.770075\n",
      "KNeighborsClassifier();RobustScaler();0.724671;0.758394\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.RidgeClassifier(),\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    ensemble.RandomForestClassifier(min_samples_leaf=5, class_weight={0:2, 1:3}),\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    neighbors.KNeighborsClassifier()\n",
    "]\n",
    "\n",
    "\n",
    "scalers = [\n",
    "    None,\n",
    "    Normalizer(),\n",
    "    MinMaxScaler(),\n",
    "    StandardScaler(),\n",
    "    QuantileTransformer(),\n",
    "    RobustScaler(),\n",
    "]\n",
    "\n",
    "def score_model(X, y, model, scaler):   \n",
    "    print('%s;%s;'%(model, scaler), end='')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "    if scaler is not None:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    # Binarize maybe\n",
    "    y_hat[y_hat >= 0.5] = 1\n",
    "    y_hat[y_hat < 0.5] = 0\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_hat)\n",
    "    if 'predict_proba' in dir(model):\n",
    "        roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "    else:\n",
    "        roc_score = roc_auc_score(y_test, model.predict(X_test))\n",
    "        \n",
    "    print(\"%f;%f\"%(accuracy,roc_score))\n",
    "\n",
    "def cross_validate(X, y, model, scaler):\n",
    "    scores = []\n",
    "    print('%s;%s;'%(model, scaler), end='')\n",
    "    \n",
    "    for i in range(4):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=i)\n",
    "\n",
    "        if scaler is not None:\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Binarize maybe\n",
    "        y_hat[y_hat >= 0.5] = 1\n",
    "        y_hat[y_hat < 0.5] = 0\n",
    "        \n",
    "        scores += [model.score(X_test, y_test)]\n",
    "    final_score = sum(scores) / len(scores)\n",
    "    print(final_score)\n",
    "    \n",
    "for model in models:\n",
    "    for scaler in scalers:\n",
    "        score_model(X, y, model, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722523091131483"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "model = ensemble.RandomForestClassifier()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_hat))\n",
    "print('AUC Score', roc_auc_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7719749216300941\n",
      "AUC Score 0.722523091131483\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy', accuracy_score(y_test, y_hat))\n",
    "print('AUC Score', roc_auc_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_OWNPV_CHI2</th>\n",
       "      <th>B_IPCHI2_OWNPV</th>\n",
       "      <th>B_FDCHI2_OWNPV</th>\n",
       "      <th>B_DIRA_OWNPV</th>\n",
       "      <th>B_PT</th>\n",
       "      <th>Kst_892_0_IP_OWNPV</th>\n",
       "      <th>Kst_892_0_cosThetaH</th>\n",
       "      <th>Kplus_IP_OWNPV</th>\n",
       "      <th>Kplus_P</th>\n",
       "      <th>piminus_IP_OWNPV</th>\n",
       "      <th>piminus_P</th>\n",
       "      <th>gamma_PT</th>\n",
       "      <th>piminus_ETA</th>\n",
       "      <th>Kplus_ETA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180762.000000</td>\n",
       "      <td>1.807620e+05</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>1.807620e+05</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>1.807620e+05</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>1.807620e+05</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>1.807620e+05</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>180762.000000</td>\n",
       "      <td>180762.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000938</td>\n",
       "      <td>8.451895e-05</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>3.013230e-05</td>\n",
       "      <td>0.257533</td>\n",
       "      <td>1.712249e-05</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.738378e-05</td>\n",
       "      <td>0.681678</td>\n",
       "      <td>1.765928e-05</td>\n",
       "      <td>0.558923</td>\n",
       "      <td>0.168696</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000700</td>\n",
       "      <td>8.313704e-05</td>\n",
       "      <td>0.139527</td>\n",
       "      <td>1.453395e-05</td>\n",
       "      <td>0.122812</td>\n",
       "      <td>1.494780e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1.567776e-05</td>\n",
       "      <td>0.186436</td>\n",
       "      <td>1.597446e-05</td>\n",
       "      <td>0.207707</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.903538e-10</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>5.073240e-07</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>4.639057e-07</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>2.664117e-07</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>4.143417e-07</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000433</td>\n",
       "      <td>2.295012e-05</td>\n",
       "      <td>0.010175</td>\n",
       "      <td>1.867240e-05</td>\n",
       "      <td>0.160016</td>\n",
       "      <td>7.213157e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>7.111546e-06</td>\n",
       "      <td>0.530055</td>\n",
       "      <td>7.219154e-06</td>\n",
       "      <td>0.389654</td>\n",
       "      <td>0.093428</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000756</td>\n",
       "      <td>5.845566e-05</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>2.751129e-05</td>\n",
       "      <td>0.238416</td>\n",
       "      <td>1.225738e-05</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.226963e-05</td>\n",
       "      <td>0.698846</td>\n",
       "      <td>1.249709e-05</td>\n",
       "      <td>0.569460</td>\n",
       "      <td>0.146518</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>1.192279e-04</td>\n",
       "      <td>0.073853</td>\n",
       "      <td>3.910674e-05</td>\n",
       "      <td>0.336636</td>\n",
       "      <td>2.157760e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>2.187809e-05</td>\n",
       "      <td>0.844323</td>\n",
       "      <td>2.219600e-05</td>\n",
       "      <td>0.737950</td>\n",
       "      <td>0.221546</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007259</td>\n",
       "      <td>7.021951e-04</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.012164e-04</td>\n",
       "      <td>0.715704</td>\n",
       "      <td>1.945528e-04</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>2.834155e-04</td>\n",
       "      <td>0.992423</td>\n",
       "      <td>2.834155e-04</td>\n",
       "      <td>0.943568</td>\n",
       "      <td>0.662169</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        B_OWNPV_CHI2  B_IPCHI2_OWNPV  B_FDCHI2_OWNPV  B_DIRA_OWNPV  \\\n",
       "count  180762.000000    1.807620e+05   180762.000000  1.807620e+05   \n",
       "mean        0.000938    8.451895e-05        0.077699  3.013230e-05   \n",
       "std         0.000700    8.313704e-05        0.139527  1.453395e-05   \n",
       "min         0.000006    3.903538e-10        0.000406  5.073240e-07   \n",
       "25%         0.000433    2.295012e-05        0.010175  1.867240e-05   \n",
       "50%         0.000756    5.845566e-05        0.024951  2.751129e-05   \n",
       "75%         0.001254    1.192279e-04        0.073853  3.910674e-05   \n",
       "max         0.007259    7.021951e-04        0.999980  1.012164e-04   \n",
       "\n",
       "                B_PT  Kst_892_0_IP_OWNPV  Kst_892_0_cosThetaH  Kplus_IP_OWNPV  \\\n",
       "count  180762.000000        1.807620e+05        180762.000000    1.807620e+05   \n",
       "mean        0.257533        1.712249e-05             0.000007    1.738378e-05   \n",
       "std         0.122812        1.494780e-05             0.000017    1.567776e-05   \n",
       "min         0.003455        4.639057e-07            -0.000062    2.664117e-07   \n",
       "25%         0.160016        7.213157e-06            -0.000004    7.111546e-06   \n",
       "50%         0.238416        1.225738e-05             0.000006    1.226963e-05   \n",
       "75%         0.336636        2.157760e-05             0.000017    2.187809e-05   \n",
       "max         0.715704        1.945528e-04             0.000092    2.834155e-04   \n",
       "\n",
       "             Kplus_P  piminus_IP_OWNPV      piminus_P       gamma_PT  \\\n",
       "count  180762.000000      1.807620e+05  180762.000000  180762.000000   \n",
       "mean        0.681678      1.765928e-05       0.558923       0.168696   \n",
       "std         0.186436      1.597446e-05       0.207707       0.098239   \n",
       "min         0.003170      4.143417e-07       0.002804       0.002355   \n",
       "25%         0.530055      7.219154e-06       0.389654       0.093428   \n",
       "50%         0.698846      1.249709e-05       0.569460       0.146518   \n",
       "75%         0.844323      2.219600e-05       0.737950       0.221546   \n",
       "max         0.992423      2.834155e-04       0.943568       0.662169   \n",
       "\n",
       "         piminus_ETA      Kplus_ETA  \n",
       "count  180762.000000  180762.000000  \n",
       "mean        0.000091       0.000091  \n",
       "std         0.000037       0.000036  \n",
       "min         0.000001       0.000001  \n",
       "25%         0.000062       0.000063  \n",
       "50%         0.000086       0.000087  \n",
       "75%         0.000115       0.000115  \n",
       "max         0.000300       0.000273  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = Normalizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "X_train_s = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_s = pd.DataFrame(data=scaler.transform(X_test), columns=X_train.columns)\n",
    "\n",
    "X_train_s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.555%\n"
     ]
    }
   ],
   "source": [
    "err = np.abs(y_hat - y_test.values)\n",
    "\n",
    "acc = (y_test.count() - err.sum()) / y_test.count()\n",
    "print(\"Accuracy: %.3f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer()\n",
      "RandomForestClassifier()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7655485893416928"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scaler)\n",
    "print(ln_model)\n",
    "ln_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Plots\n",
    "\n",
    "We can plot the data using Seaborn's \"Pair Plotting\" to get a very nice visualization of correlation graphs for each variable against each other, as well as density graphs for cases where the prediction variable is one value or the other (the \"hue\" parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col=0)\n",
    "df.columns = df.columns.str.strip()\n",
    "del df['BUTTER']\n",
    "\n",
    "sns.pairplot(df[::1000], hue='signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observe a couple of interesting facts about our features:**\n",
    "\n",
    "1. There is an extremely suspicious variable called `Butter` that separates almost perfectly the two groups, better than any other feature. If we take a look at it's description we get: `BUTTER: Corresponds to the relative consumption of butter cream in Switzerland.` Nonsense. It is probably a trap and the data looks generated by a standard normal distribution with posterior knowledge of the \"signal\" target. **We should discard it**.\n",
    "\n",
    "\n",
    "2. Some of the features **completely overlap their density graphs on both signals 1 and 0**. These features can at best be ignored by our model and at worst cause numerical instability or bias errors. **We should try to train the models with these features on or off** to see which case performs better.\n",
    "\n",
    "\n",
    "3. Some features show promise of having a good correlation with signal. We can observe this when the Pair plots have **distinct colored clusters** and their **density graphs don't overlap** in some areas.\n",
    "\n",
    "\n",
    "4. Could we do some post-processing of the features showing an exponential distribution to bring into a log-normalized space? Would that help prediction accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
